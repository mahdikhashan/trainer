{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd6b9252-2470-4928-b7c1-a238f818c8c0",
   "metadata": {},
   "source": [
    "# Fine-Tune Llama 3.2-3B with MLX Distributed and LoRA\n",
    "\n",
    "This Notebook will show how to fine-tune Llama 3.2-3B on multiple GPUs and `mlx-lm`.\n",
    "\n",
    "The MLX runtime uses `mlx[cuda]` package to run distributed training on GPUs.\n",
    "\n",
    "MLX Distributed: https://ml-explore.github.io/mlx/build/html/usage/distributed.html\n",
    "\n",
    "MLX LM: https://github.com/ml-explore/mlx-lm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2101794-2700-499f-ab14-7798e8177d02",
   "metadata": {},
   "source": [
    "## Install the Kubeflow SDK\n",
    "\n",
    "You need to install the Kubeflow SDK to interact with Kubeflow Trainer APIs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59976ad2-0f80-4fc5-8fae-917fa0dc0841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/kubeflow/sdk.git@main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fcf0b5-0641-424b-93f6-f10a1b35c172",
   "metadata": {},
   "source": [
    "## Update the GPU Resources\n",
    "\n",
    "Currently, Kubeflow Trainer does not support configuring MLX resources directly through a\n",
    "TrainJob specification.\n",
    "\n",
    "To adjust GPU allocations (and other container resource settings), you must manually patch the ClusterTrainingRuntime.\n",
    "\n",
    "Progress for native resource configuration in TrainJob is being tracked here: [kubeflow/trainer#2650](https://github.com/kubeflow/trainer/issues/2650)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c09c977-3679-4d33-94b5-89a476e055c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clustertrainingruntime.trainer.kubeflow.org/mlx-distributed patched (no change)\n"
     ]
    }
   ],
   "source": [
    "patch = \"\"\"\n",
    "[\n",
    "  {\n",
    "    \"op\": \"add\",\n",
    "    \"path\": \"/spec/template/spec/replicatedJobs/0/template/spec/template/spec/containers/0/resources\",\n",
    "    \"value\": { \"limits\": { \"nvidia.com/gpu\": \"1\" } }\n",
    "  },\n",
    "  {\n",
    "    \"op\": \"add\",\n",
    "    \"path\": \"/spec/template/spec/replicatedJobs/1/template/spec/template/spec/containers/0/resources\",\n",
    "    \"value\": { \"limits\": { \"nvidia.com/gpu\": \"1\" } }\n",
    "  }\n",
    "]\n",
    "\"\"\"\n",
    "!kubectl patch clustertrainingruntime mlx-distributed --type='json' -p \"$patch\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b85fb2-9572-4903-8cf1-930c4844aabd",
   "metadata": {},
   "source": [
    "## Create Script to Fine-Tune Llama 3.2\n",
    "\n",
    "We will use `mlx-lm` library to fine-tune Llama 3.2.\n",
    "\n",
    "`mlx-lm` is a Python package for generating text and fine-tuning LLMs with MLX.\n",
    "\n",
    "We will perform LoRA (Low-Rank Adaptation) fine-tuning to reduce number of trainable parameters and optimize GPU resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f626231c-3f49-40d6-bd37-6d8aff83cede",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_llama(func_args):\n",
    "    import types\n",
    "    import os\n",
    "    import mlx.core as mx\n",
    "    from mlx_lm.lora import train_model, CONFIG_DEFAULTS\n",
    "    from mlx_lm.tuner.datasets import load_dataset\n",
    "    from mlx_lm.utils import load\n",
    "    from mlx_lm.generate import generate\n",
    "\n",
    "    os.environ[\"HF_TOKEN\"] = func_args[\"HF_TOKEN\"]\n",
    "\n",
    "    # Set parameters for the mlx-lm.\n",
    "    args = types.SimpleNamespace()\n",
    "    args.model = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "    args.data = \"mlx-community/WikiSQL\"\n",
    "    args.train = True\n",
    "    # Configure LoRA settings to reduce number of trainable params.\n",
    "    args.lora_parameters = {\n",
    "        \"rank\": 8,\n",
    "        \"dropout\": 0.05,\n",
    "        \"scale\": 20.0,\n",
    "    }\n",
    "\n",
    "    args.iters = int(func_args[\"NUM_SAMPLES\"])\n",
    "    args.batch_size = int(func_args[\"BATCH_SIZE\"])\n",
    "\n",
    "    # Set defaults for other required parameters\n",
    "    for k, v in CONFIG_DEFAULTS.items():\n",
    "        if not hasattr(args, k):\n",
    "            setattr(args, k, v)\n",
    "\n",
    "    model, tokenizer = load(args.model)\n",
    "    train_set, valid_set, test_set = load_dataset(args, tokenizer)\n",
    "\n",
    "    # Start the Llama distributed fine-tuning.\n",
    "    train_model(args, model, train_set, valid_set)\n",
    "\n",
    "    # Evaluate the fine-tuned adapter.\n",
    "    dist = mx.distributed.init(strict=True, backend=\"mpi\")\n",
    "    if dist.rank() == 0:\n",
    "        print(\"=\" * 100)\n",
    "        print(f\"Training is complete. Adapters saved to: {args.adapter_path}\")\n",
    "        print(\"Evaluate the fine-tuned LoRA adapter for Llama 3.2\")\n",
    "\n",
    "        finetuned_model, finetuned_tokenizer = load(\n",
    "            args.model, adapter_path=args.adapter_path\n",
    "        )\n",
    "\n",
    "        # Generate response using the fine-tuned adapter.\n",
    "        sample_prompt = \"What is SQL?\"\n",
    "\n",
    "        print(f\"Prompt: {sample_prompt}\")\n",
    "        print(\"Response:\")\n",
    "\n",
    "        response = generate(\n",
    "            model=finetuned_model,\n",
    "            tokenizer=finetuned_tokenizer,\n",
    "            prompt=sample_prompt,\n",
    "            max_tokens=1000,\n",
    "            verbose=False,\n",
    "        )\n",
    "\n",
    "        print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48096ce0-33f3-4249-b1cb-b83e5ee9d258",
   "metadata": {},
   "source": [
    "## Get the MLX Runtime\n",
    "\n",
    "You can list the available Kubeflow Trainer runtimes with the `list_runtimes()` API.\n",
    "\n",
    "The name of the MLX runtime is `mlx-distributed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be6a71c0-d61e-442a-93e7-211d93942de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: mlx-distributed, Framework: mlx, Trainer Type: CustomTrainer\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from kubeflow.trainer import TrainerClient, CustomTrainer\n",
    "\n",
    "for r in TrainerClient().list_runtimes():\n",
    "    if r.name == \"mlx-distributed\":\n",
    "        print(f\"Name: {r.name}, Framework: {r.trainer.framework}, Trainer Type: {r.trainer.trainer_type.value}\\n\")\n",
    "        mlx_runtime = r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe68e70-74ee-4c38-ae3e-8c5b214e3e91",
   "metadata": {},
   "source": [
    "## Get the Runtime Packages\n",
    "\n",
    "You can see the available Python packages and GPUs with the `get_runtime_packages()` API.\n",
    "\n",
    "The API shows available GPUs with CUDA driver on the single training node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf65074b-7170-4a14-a9f8-3ea9420f090e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Permanently added '[o2d08d18ed37-node-0-0.o2d08d18ed37]:2222' (ECDSA) to the list of known hosts.\n",
      "Python: 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]\n",
      "Package                Version\n",
      "---------------------- -----------\n",
      "aiohappyeyeballs       2.6.1\n",
      "aiohttp                3.12.15\n",
      "aiosignal              1.4.0\n",
      "async-timeout          5.0.1\n",
      "attrs                  25.3.0\n",
      "certifi                2025.8.3\n",
      "charset-normalizer     3.4.3\n",
      "datasets               4.0.0\n",
      "dill                   0.3.8\n",
      "filelock               3.19.1\n",
      "frozenlist             1.7.0\n",
      "fsspec                 2025.3.0\n",
      "hf-xet                 1.1.7\n",
      "huggingface-hub        0.34.4\n",
      "idna                   3.10\n",
      "Jinja2                 3.1.6\n",
      "MarkupSafe             3.0.2\n",
      "mlx                    0.28.0\n",
      "mlx-cuda               0.28.0\n",
      "mlx-data               0.1.0\n",
      "mlx-lm                 0.26.3\n",
      "multidict              6.6.4\n",
      "multiprocess           0.70.16\n",
      "numpy                  2.2.6\n",
      "nvidia-cublas-cu12     12.9.1.4\n",
      "nvidia-cuda-nvrtc-cu12 12.9.86\n",
      "nvidia-cudnn-cu12      9.12.0.46\n",
      "packaging              25.0\n",
      "pandas                 2.3.1\n",
      "pip                    22.0.2\n",
      "propcache              0.3.2\n",
      "protobuf               6.32.0\n",
      "pyarrow                21.0.0\n",
      "python-dateutil        2.9.0.post0\n",
      "pytz                   2025.2\n",
      "PyYAML                 6.0.2\n",
      "regex                  2025.7.34\n",
      "requests               2.32.4\n",
      "safetensors            0.6.2\n",
      "setuptools             59.6.0\n",
      "six                    1.17.0\n",
      "tokenizers             0.21.4\n",
      "tqdm                   4.67.1\n",
      "transformers           4.55.2\n",
      "typing_extensions      4.14.1\n",
      "tzdata                 2025.2\n",
      "urllib3                2.5.0\n",
      "wheel                  0.37.1\n",
      "xxhash                 3.5.0\n",
      "yarl                   1.20.1\n",
      "\n",
      "Available GPUs on the single training node\n",
      "Tue Aug 19 11:32:32 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.163.01             Driver Version: 550.163.01     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100-SXM4-40GB          On  |   00000000:20:1D.0 Off |                    0 |\n",
      "| N/A   44C    P0             58W /  400W |       1MiB /  40960MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TrainerClient().get_runtime_packages(mlx_runtime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54ed60e-21bb-4184-963a-c1c690da1f1e",
   "metadata": {},
   "source": [
    "## Create TrainJob with MLX Distributed\n",
    "\n",
    "Use the `train()` API to create distributed TrainJob on **4 GPUs**. Every MPI training node uses 1 GPU.\n",
    "\n",
    "**Note** Update the HF Token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32a30359-d275-4e4a-b714-da9dae5d5358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HF_TOKEN = \"hf_add_your_token\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c87e3d06-1cbf-4e14-acad-10a7dc2292a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"HF_TOKEN\": HF_TOKEN,\n",
    "    \"NUM_SAMPLES\": \"100\",\n",
    "    # Batch size must be divisible by the number of GPUs. (8 / 4 = 2) per training node.\n",
    "    \"BATCH_SIZE\": \"8\",\n",
    "}\n",
    "\n",
    "job_id = TrainerClient().train(\n",
    "    trainer=CustomTrainer(\n",
    "        func=fine_tune_llama,\n",
    "        func_args=args,\n",
    "        num_nodes=4,  # Fine-Tune Llama3.2 on 4 GPUs.\n",
    "    ),\n",
    "    runtime=mlx_runtime,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c526b36-3772-4512-b2e4-b794d600b384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a44128b165b2'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train API generates a random TrainJob id.\n",
    "job_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6952099-0e24-4f48-9520-10cc90b5d0b9",
   "metadata": {},
   "source": [
    "## Check the TrainJob Info\n",
    "\n",
    "Use the `list_jobs()` and `get_job()` APIs to get information about created TrainJob and its steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05477438-0e23-45e3-92f3-76cfeff5a31c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainJob: a44128b165b2, Status: Running, Created at: 2025-08-18 14:42:06+00:00\n"
     ]
    }
   ],
   "source": [
    "for job in TrainerClient().list_jobs():\n",
    "    print(f\"TrainJob: {job.name}, Status: {job.status}, Created at: {job.creation_timestamp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0af3dd0-ae67-4b77-9154-ce46326b727f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: node-0, Status: Running, Devices: gpu x 1\n",
      "\n",
      "Step: node-1, Status: Running, Devices: gpu x 1\n",
      "\n",
      "Step: node-2, Status: Running, Devices: gpu x 1\n",
      "\n",
      "Step: node-3, Status: Running, Devices: gpu x 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We execute mpirun command on node-0, which functions as the MPI Launcher node.\n",
    "for c in TrainerClient().get_job(name=job_id).steps:\n",
    "    print(f\"Step: {c.name}, Status: {c.status}, Devices: {c.device} x {c.device_count}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0e85af-9dab-4517-944f-72c6625ed0c9",
   "metadata": {},
   "source": [
    "## Get the TrainJob Logs\n",
    "\n",
    "Use the `get_job_logs()` API to retrieve the TrainJob logs.\n",
    "\n",
    "The fine-tuning runs on 4 training nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4be42866-b688-427d-a9a3-fe8e0aff5c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[node-0]: Warning: Permanently added '[a44128b165b2-node-0-0.a44128b165b2]:2222' (ECDSA) to the list of known hosts.\n",
      "[node-0]: Warning: Permanently added '[a44128b165b2-node-0-2.a44128b165b2]:2222' (ECDSA) to the list of known hosts.\n",
      "[node-0]: Warning: Permanently added '[a44128b165b2-node-0-1.a44128b165b2]:2222' (ECDSA) to the list of known hosts.\n",
      "Fetching 11 files: 100%|██████████| 11/11 [00:08<00:00,  1.31it/s]\n",
      "Fetching 11 files: 100%|██████████| 11/11 [00:08<00:00,  1.25it/s]\n",
      "Fetching 11 files: 100%|██████████| 11/11 [00:08<00:00,  1.23it/s]\n",
      "Fetching 11 files: 100%|██████████| 11/11 [00:09<00:00,  1.22it/s]\n",
      "[node-0]: Loading Hugging Face dataset mlx-community/WikiSQL.\n",
      "[node-0]: Loading Hugging Face dataset mlx-community/WikiSQL.\n",
      "[node-0]: Loading Hugging Face dataset mlx-community/WikiSQL.\n",
      "[node-0]: Loading Hugging Face dataset mlx-community/WikiSQL.\n",
      "Generating train split: 100%|██████████| 1000/1000 [00:00<00:00, 274030.05 examples/s]\n",
      "Generating valid split: 100%|██████████| 100/100 [00:00<00:00, 62723.25 examples/s]\n",
      "Generating test split: 100%|██████████| 100/100 [00:00<00:00, 93289.68 examples/s]\n",
      "[node-0]: Trainable parameters: 0.041% (1.311M/3212.750M)\n",
      "[node-0]: Starting training..., iters: 100\n",
      "Generating train split: 100%|██████████| 1000/1000 [00:00<00:00, 295311.13 examples/s]\n",
      "Generating valid split: 100%|██████████| 100/100 [00:00<00:00, 62629.60 examples/s]\n",
      "Generating test split: 100%|██████████| 100/100 [00:00<00:00, 86695.00 examples/s]\n",
      "Generating train split: 100%|██████████| 1000/1000 [00:00<00:00, 294337.12 examples/s]\n",
      "Generating valid split: 100%|██████████| 100/100 [00:00<00:00, 80473.98 examples/s]\n",
      "Generating test split: 100%|██████████| 100/100 [00:00<00:00, 84922.13 examples/s]\n",
      "[node-0]: Trainable parameters: 0.041% (1.311M/3212.750M)\n",
      "[node-0]: Starting training..., iters: 100\n",
      "[node-0]: Trainable parameters: 0.041% (1.311M/3212.750M)\n",
      "[node-0]: Starting training..., iters: 100\n",
      "Generating train split: 100%|██████████| 1000/1000 [00:00<00:00, 278858.05 examples/s]\n",
      "Generating valid split: 100%|██████████| 100/100 [00:00<00:00, 82016.11 examples/s]\n",
      "Generating test split: 100%|██████████| 100/100 [00:00<00:00, 91538.72 examples/s]\n",
      "[node-0]: Trainable parameters: 0.041% (1.311M/3212.750M)\n",
      "[node-0]: Starting training..., iters: 100\n",
      "[node-0]: Node 0 of 4\n",
      "[node-0]: Node 2 of 4\n",
      "[node-0]: Node 3 of 4\n",
      "[node-0]: Node 1 of 4\n",
      "Calculating loss...: 100%|██████████| 12/12 [00:09<00:00,  1.23it/s]\n",
      "Calculating loss...: 100%|██████████| 12/12 [00:09<00:00,  1.23it/s]\n",
      "Calculating loss...: 100%|██████████| 12/12 [00:09<00:00,  1.23it/s]\n",
      "Calculating loss...: 100%|██████████| 12/12 [00:10<00:00,  1.20it/s]\n",
      "[node-0]: Iter 1: Val loss 2.928, Val took 10.023s\n",
      "[node-0]: Iter 10: Train loss 2.733, Learning Rate 1.000e-05, It/sec 0.150, Tokens/sec 96.706, Trained Tokens 6462, Peak mem 8.136 GB\n",
      "[node-0]: Iter 20: Train loss 2.105, Learning Rate 1.000e-05, It/sec 15.556, Tokens/sec 9722.253, Trained Tokens 12712, Peak mem 8.136 GB\n",
      "[node-0]: Iter 30: Train loss 1.707, Learning Rate 1.000e-05, It/sec 21.797, Tokens/sec 13773.815, Trained Tokens 19031, Peak mem 8.136 GB\n",
      "[node-0]: Iter 40: Train loss 1.648, Learning Rate 1.000e-05, It/sec 16.987, Tokens/sec 11172.488, Trained Tokens 25608, Peak mem 8.136 GB\n",
      "[node-0]: Iter 50: Train loss 1.509, Learning Rate 1.000e-05, It/sec 13.261, Tokens/sec 8316.022, Trained Tokens 31879, Peak mem 8.581 GB\n",
      "[node-0]: Iter 60: Train loss 1.537, Learning Rate 1.000e-05, It/sec 16.607, Tokens/sec 11002.177, Trained Tokens 38504, Peak mem 8.581 GB\n",
      "[node-0]: Iter 70: Train loss 1.508, Learning Rate 1.000e-05, It/sec 23.419, Tokens/sec 14814.577, Trained Tokens 44830, Peak mem 8.581 GB\n",
      "[node-0]: Iter 80: Train loss 1.489, Learning Rate 1.000e-05, It/sec 13.253, Tokens/sec 8507.290, Trained Tokens 51249, Peak mem 8.581 GB\n",
      "[node-0]: Iter 90: Train loss 1.410, Learning Rate 1.000e-05, It/sec 25.012, Tokens/sec 15947.385, Trained Tokens 57625, Peak mem 8.581 GB\n",
      "Calculating loss...: 100%|██████████| 12/12 [00:00<00:00, 59.13it/s]\n",
      "Calculating loss...: 100%|██████████| 12/12 [00:00<00:00, 58.71it/s]\n",
      "Calculating loss...: 100%|██████████| 12/12 [00:00<00:00, 58.67it/s]\n",
      "Calculating loss...: 100%|██████████| 12/12 [00:00<00:00, 58.03it/s]\n",
      "[node-0]: Iter 100: Val loss 1.481, Val took 0.213s\n",
      "[node-0]: Iter 100: Train loss 1.423, Learning Rate 1.000e-05, It/sec 23.717, Tokens/sec 15373.658, Trained Tokens 64107, Peak mem 8.581 GB\n",
      "[node-0]: Iter 100: Saved adapter weights to adapters/adapters.safetensors and adapters/0000100_adapters.safetensors.\n",
      "[node-0]: Saved final weights to adapters/adapters.safetensors.\n",
      "[node-0]: ====================================================================================================\n",
      "[node-0]: Training is complete. Adapters saved to: adapters\n",
      "[node-0]: Evaluate the fine-tuned LoRA adapter for Llama 3.2\n",
      "Fetching 11 files: 100%|██████████| 11/11 [00:00<00:00, 40685.49it/s]\n",
      "[node-0]: Prompt: What is SQL?\n",
      "[node-0]: Response:\n",
      "[node-0]: SQL (Structured Query Language) is a programming language designed for managing and manipulating data stored in relational database management systems (RDBMS). It is used to perform various operations such as creating, modifying, and querying databases.\n",
      "[node-0]: SQL is a standard language for accessing, managing, and modifying data in relational database management systems. It is used to perform various operations such as creating, modifying, and querying databases. SQL is used to create, modify, and query databases, and it is the standard language for relational database management systems.\n",
      "[node-0]: SQL is used to perform various operations such as:\n",
      "[node-0]: * Creating tables and indexes\n",
      "[node-0]: * Modifying data in tables\n",
      "[node-0]: * Querying data in tables\n",
      "[node-0]: * Inserting, updating, and deleting data\n",
      "[node-0]: * Creating views and stored procedures\n",
      "[node-0]: * Controlling access to data using permissions and access control lists (ACLs)\n",
      "[node-0]: \n",
      "[node-0]: SQL is used in a variety of applications, including:\n",
      "[node-0]: * Web applications\n",
      "[node-0]: * Mobile applications\n",
      "[node-0]: * Desktop applications\n",
      "[node-0]: * Enterprise applications\n",
      "[node-0]: * Data analysis and reporting\n",
      "[node-0]: \n",
      "[node-0]: SQL is a powerful tool for managing and manipulating data, and it is widely used in many different applications.\n",
      "[node-0]: \n",
      "[node-0]: **Types of SQL:**\n",
      "[node-0]: \n",
      "[node-0]: * **T-SQL (Transact-SQL):** Used for Microsoft SQL Server\n",
      "[node-0]: * **PL/SQL (Procedural Language/SQL):** Used for Oracle\n",
      "[node-0]: * **MySQL:** Used for MySQL database\n",
      "[node-0]: * **PostgreSQL:** Used for PostgreSQL database\n",
      "[node-0]: * **SQLite:** Used for SQLite database\n",
      "[node-0]: \n",
      "[node-0]: **SQL Syntax:**\n",
      "[node-0]: \n",
      "[node-0]: * **SELECT:** Used to select data from a table\n",
      "[node-0]: * **INSERT:** Used to insert new data into a table\n",
      "[node-0]: * **UPDATE:** Used to modify existing data in a table\n",
      "[node-0]: * **DELETE:** Used to delete data from a table\n",
      "[node-0]: * **CREATE:** Used to create a new table or database\n",
      "[node-0]: * **DROP:** Used to delete a table or database\n",
      "[node-0]: * **ALTER:** Used to modify the structure of a table\n",
      "[node-0]: \n",
      "[node-0]: **SQL Query:**\n",
      "[node-0]: \n",
      "[node-0]: * **SELECT:** `SELECT * FROM table_name WHERE column_name = 'value'`\n",
      "[node-0]: * **INSERT:** `INSERT INTO table_name (column1, column2) VALUES ('value1', 'value2')`\n",
      "[node-0]: * **UPDATE:** `UPDATE table_name SET column1 = 'value' WHERE column_name = 'value'`\n",
      "[node-0]: * **DELETE:** `DELETE FROM table_name WHERE column_name = 'value'`\n",
      "[node-0]: \n",
      "[node-0]: **SQL Best Practices:**\n",
      "[node-0]: \n",
      "[node-0]: * **Use meaningful table and column names**\n",
      "[node-0]: * **Use indexes to improve query performance**\n",
      "[node-0]: * **Use transactions to ensure data consistency**\n",
      "[node-0]: * **Use stored procedures to encapsulate complex logic**\n",
      "[node-0]: * **Use views to simplify complex queries**\n",
      "[node-0]: \n",
      "[node-0]: **SQL Resources:**\n",
      "[node-0]: \n",
      "[node-0]: * **W3Schools SQL Tutorial:** A comprehensive tutorial for learning SQL\n",
      "[node-0]: * **SQLCourse:** A free online SQL course\n",
      "[node-0]: * **SQL Fiddle:** A tool for creating and sharing SQL queries\n",
      "[node-0]: * **Stack Overflow:** A Q&A platform for SQL and programming-related questions\n",
      "[node-0]: \n",
      "[node-0]: I hope this helps! Let me know if you have any questions or need further clarification.\n"
     ]
    }
   ],
   "source": [
    "_ = TrainerClient().get_job_logs(name=job_id, follow=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff39226-e207-4daa-87e7-60ac4dd5869d",
   "metadata": {},
   "source": [
    "## Delete the TrainJob\n",
    "\n",
    "When TrainJob is finished, you can delete the resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53978afb-d4e8-4e06-91af-fe6e3538d211",
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainerClient().delete_job(job_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
